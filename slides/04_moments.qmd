---
title: "確率論"
subtitle: "期待値、分散、モーメント"
author: "森 立平"
format:
  revealjs:
    slide-number: true
    smaller: true
    html-math-method: mathjax
    include-in-header: macros.qmd
css: slides.css
---

## 期待値

::: {#def-exp}
## 期待値

離散型確率変数 $X$ の**期待値**は

\begin{align*}
\expt{X}
% &= \sum_{\omega \in \Omega} X(\omega) P(\{\omega\})\\
%&= \sum_{x\in \mathrm{Image}(X)} x P(\{\omega\mid X(\omega)=x\})\\
&\coloneqq \sum_{x\in \mathrm{Image}(X)} x \Pr(X=x)\\
&= \sum_{x\in \mathrm{Image}(X)} x f_X(x)
\end{align*}

と定義される。ここで、**右辺の和が絶対収束しない場合は(適当な順番で和を取って収束したとしても)期待値は定義されない**。

連続型確率変数 $X$ が確率密度関数 $f_X$ を持つとき、その**期待値**は

$$
\expt{X}\coloneqq \int_{-\infty}^\infty x f_X(x) \mathrm{d}x
$$

と定義される。
ただし、広義積分で上記の積分が存在する場合でも、
$$
\int_{-\infty}^\infty |x| f_X(x) \mathrm{d}x
$$
が存在しない場合には期待値は定義されない。
:::

連続型確率変数の期待値に関する様々な証明は**ルベーグ積分の知識を必要とするのでこの授業では扱わない**。
以下、証明はすべて**離散確率変数の場合に限って**与えて分かった気になることにする。

## 確率変数の和の期待値

::: {#lem-exp-sum}
確率変数 $X,\,Y$ について
\begin{align*}
\expt{X+Y}&=\expt{X}+\expt{Y}.
\end{align*}
:::
::: {.proof}
\begin{align*}
\expt{X+Y} &= \sum_{z} z f_{X+Y}(z)\\
&= \sum_{z} z \sum_x f_{X,\,Y}(x, z-x)\\
&= \sum_{x,\,y} (x+y) f_{X,\,Y}(x, y)\qquad (y=z-x)\\
&= \sum_{x,\,y} xf_{X,\,Y}(x,y) + \sum_{x,\,y} yf_{X,\,Y}(x,y)\\
&= \sum_{x} xf_{X}(x) + \sum_{y} yf_{Y}(y)\\
&= \expt{X} + \expt{Y}.
\end{align*}
:::

## 確率変数の積の期待値

::: {#lem-exp-prod}
確率変数 $X$ と $Y$ が**独立**のとき、
\begin{align*}
\expt{XY}&=\expt{X}\,\expt{Y}.
\end{align*}
:::
::: {.proof}
\begin{align*}
\expt{XY} &= \sum_{z} z f_{XY}(z)\\
&= \sum_{z} z \sum_{x\ne 0}f_{X,\,Y}(x,z/x)\\
&= \sum_{z} z \sum_{x\ne 0}f_{X}(x)f_Y(z/x)\\
&= \sum_{x\ne 0,\, y} xy f_{X}(x)f_Y(y)\qquad(y=z/x)\\
&= \sum_{x,\, y} xy f_{X}(x)f_Y(y)\\
&= \left(\sum_{x} xf_{X}(x)\right)\left(\sum_y yf_Y(y)\right)\\
&= \expt{X}\,\expt{Y}
\end{align*}
:::

## 期待値の簡潔な表現
::: {#lem-lotus}
### Law of the unconscious statistician (LOTUS)
任意の関数 $g\colon\mathbb{R}\to\mathbb{R}$ について、

1. $X$ が離散型確率変数のとき、
\begin{align*}
\expt{g(X)} &= \sum_{x} g(x) f_X(x).
\end{align*}
1. $X$ が連続型確率変数で確率密度関数を持つとき、
\begin{align*}
\expt{g(X)} &= \int_{-\infty}^\infty g(x) f_X(x) \mathrm{d}x.
\end{align*}
:::
::: {.proof}
$X$ を離散型確率変数とする。
\begin{align*}
\expt{g(X)}
%&= \sum_{x} x f_{g(X)}(x)\\
&= \sum_{x} x \Pr(g(X) = x)
= \sum_{x} x P(\{\omega\in\Omega\mid g(X(\omega)) = x\})\\
&= \sum_{x} x P\left(\bigcup_{y\in\mathrm{Image}(X)\colon\, g(y) = x}\{\omega\in\Omega\mid X(\omega) = y\}\right)\\
&= \sum_{x}\sum_{y\in\mathrm{Image}(X)\colon\, g(y) = x} x P\left(\{\omega\in\Omega\mid X(\omega) = y\}\right)\\
&= \sum_{y\in\mathrm{Image}(X)} g(y) f_X(y).
\end{align*}
:::

## 期待値の性質
::: {#prp-exp}
## 期待値の性質

任意の確率変数 $X$ と $a\in\mathbb{R}$ について
\begin{align*}
\expt{X+a} &= \expt{X}+a\\
\expt{aX} &= a\expt{X}.
\end{align*}
:::

## マルコフの不等式
::: {#thm-markov}
## マルコフの不等式

任意の**非負**確率変数 $X$ と $a>0$ について
$$
\Pr(X\ge a)\le\frac{\expt{X}}{a}.
$$
:::
::: {.proof}
\begin{align*}
\expt{X} &= \sum_{x\in \mathrm{Image}(X)} f_X(x) x\\
&= \sum_{\mathrm{Image}(X)\colon\, x\ge a} f_X(x) x + \sum_{\mathrm{Image}(X)\colon\, x < a} f_X(x) x\\
&\ge \sum_{\mathrm{Image}(X)\colon\, x\ge a} f_X(x) x\qquad\qquad (\Pr(X\ge 0)=1)\\
&\ge \sum_{\mathrm{Image}(X)\colon\, x\ge a} f_X(x) a\\
&= \Pr(X\ge a) a.
\end{align*}
:::

## 分散
::: {#def-var}
## 分散

確率変数 $X$ が期待値を持つとき、その**分散**を

$$
\var{X}\coloneqq  \expt{(X-\expt{X})^2}
$$

と定義する。
また、分散の平方根を**標準偏差**という。
:::

確率変数 $X$ が分散を持つとき、

\begin{align*}
\var{X} &= \expt{(X-\expt{X})^2}\\
&= \expt{X^2-2X\expt{X}+\expt{X}^2}\\
&= \expt{X^2}-2\expt{X}\expt{X}+\expt{X}^2\\
&= \expt{X^2}-\expt{X}^2
\end{align*}

である。
分散は定義より非負の値を持つ。

::: {#prp-var}
## 分散の性質

任意の確率変数 $X$ と $a\in\mathbb{R}$ について
\begin{align*}
\var{X+a} &= \var{X}\\
\var{aX} &= a^2\var{X}.
\end{align*}
:::

分散は直感的には期待値からのはずれ具合を表す値である。

## チェビシェフの不等式
::: {#thm-chebyshev}
## チェビシェフの不等式

任意の確率変数 $X$ と $a>0$ について
$$
\Pr(|X-\expt{X}|\ge a)\le\frac{\var{X}}{a^2}.
$$
:::
::: {.proof}
\begin{align*}
\Pr(|X-\expt{X}|\ge a)&= \Pr((X-\expt{X})^2\ge a^2)\\
&\le
\frac{\expt{(X-\expt{X})^2}}{a^2} = \frac{\var{X}}{a^2}.
\end{align*}
:::

## 互いに独立な確率変数の和
::: {#lem-pairwise-var}
## 互いに独立な確率変数の和

確率変数 $X_1,\,X_2,\dotsc,X_n$ が互いに独立のとき

\begin{align*}
\var{X_1+\dotsb+X_n} &= \var{X_1} +\dotsb + \var{X_n}.
\end{align*}
:::
::: {.proof}
\begin{align*}
\var{X_1+\dotsb+X_n} &= \expt{\left((X_1+\dotsb+X_n) - \expt{X_1+\dotsb+X_n}\right)^2}\\
&= \expt{\left((X_1- \expt{X_1}) + \dotsb + (X_n-\expt{X_n})\right)^2}\\
&= \expt{\sum_i (X_i- \expt{X_i})^2  + 2\sum_{i < j}\left(X_i-\expt{X_i}\right)\left(X_j-\expt{X_j}\right)}\\
&= \sum_i \expt{(X_i- \expt{X_i})^2} +  2\sum_{i < j}\expt{\left(X_i-\expt{X_i}\right)\left(X_j-\expt{X_j}\right)}\\
&= \sum_i \expt{(X_i- \expt{X_i})^2} +  2\sum_{i < j}\expt{X_i-\expt{X_i}}\expt{X_j-\expt{X_j}}\\
&= \sum_i \expt{(X_i- \expt{X_i})^2}
=\sum_i \var{X_i}.
\end{align*}
:::

## 確率変数の平均
::: {#exm-pairwise}
互いに独立な確率変数 $X_1,\dotsc,X_n$ のそれぞれが確率変数 $X$ と同分布であるとし、
\begin{align*}
Y&\coloneqq \frac1{n} (X_1+\dotsb+X_n)
\end{align*}
と定義する。
このとき、
\begin{align*}
\expt{Y} &= \expt{X}\\
\var{Y} &= \frac1{n}\var{X}
\end{align*}
である。互いに独立な確率変数の平均を取ると期待値は変わらず、分散は小さくなる。

チェビシェフの不等式を適用すると、$Y$ は $n$ を大きくすると期待値周辺に集中していくことが分かる。
:::



## 共分散
::: {#def-exp}
## 共分散

確率変数 $X,Y$ が期待値を持つとき、その**共分散**を

\begin{align*}
\cov{X}{Y} &\coloneqq \expt{(X-\expt{X})(Y-\expt{Y})}\\
&= \expt{XY}-\expt{X}\,\expt{Y}
\end{align*}

と定義する。
共分散がゼロである確率変数のペアを**無相関**であるという。
:::

定義より、$\cov{X}{X}=\var{X}$ であることが分かる。

::: {#prp-ind-cov}
独立確率変数 $X,Y$ は無相関である。
:::

## 無相関だけど独立ではない例
逆に無相関であっても独立とは限らない。

::: {#exm-ind-cov}
確率変数 $X$ を
\begin{align*}
f_X(0)= f_X(+1)= f_X(-1)= \frac13
\end{align*}
を満たすものとし、$Y=X^2$ とする。
このとき、
\begin{align*}
\cov{X}{Y} &= \expt{XY} - \expt{X}\expt{Y}\\
&= \expt{X^3} - \expt{X}\expt{X^2}\\
&= \expt{X} - \expt{X}\expt{X^2}\\
&= \expt{X}(1-\expt{X^2})\\
&= 0
\end{align*}
なので、$X$ と $Y$ は無相関である。
一方で
\begin{align*}
f_{Y}(0) &= \frac13&
f_{Y}(1) &= \frac23\\
f_{X,\,Y}(0, 0) &= \frac13&
f_{X,\,Y}(1, 1) &= \frac13&
f_{X,\,Y}(-1, 1) &= \frac13&
\end{align*}
なので $X$ と $Y$ は独立ではない。
:::

## 共分散と分散の関係

共分散は正の値も負の値も取り得る。
大雑把に言うと、

* $X$ と $Y$ の共分散が正 $\iff$ $X$ が大きいとき $Y$ も大きい
* $X$ と $Y$ の共分散が負 $\iff$ $X$ が大きいとき $Y$ は小さい

という意味になる。


::: {#lem-var-sum}
任意の確率変数 $X_1,\dotsc,X_n$ について
\begin{align*}
\var{\sum_i X_i} &= \sum_i \var{X_i} + 2\sum_{i < j} \cov{X_i}{X_j}.
\end{align*}
:::
::: {.proof}
@lem-pairwise-var の証明参照。
:::

## モーメントとモーメント母関数

::: {#def-moment}
## モーメント

確率変数 $X$ と正の整数 $n\ge 1$ について、

$$
\mu_n(X)\coloneqq \expt{X^n}
$$

を $X$ の $n$ 次モーメントという。
:::

::: {#def-mg}
## モーメント母関数(積率母関数)

確率変数 $X$ について、

$$
M_X(t) \coloneqq \expt{\mathrm{e}^{tX}}\qquad t\in\mathbb{R}
$$

を $X$ の**モーメント母関数**という。
すべての $t\in\mathbb{R}$ で $M_X(t)$ が存在しない場合もある。
また、

$$
K_X(t) \coloneqq \log M_X(t)
$$
を $X$ の**キュムラント母関数**という。
:::

<br>

定義より、$M_X(0) = 1$, $K_X(0)=0$ である。

また、**独立**確率変数 $X,\,Y$ について
\begin{align*}
M_{X+Y}(t) &= \expt{\mathrm{e}^{t(X+Y)}} = \expt{\mathrm{e}^{tX}\cdot\mathrm{e}^{tY}}
= \expt{\mathrm{e}^{tX}}\cdot\expt{\mathrm{e}^{tY}} = M_X(t)M_Y(t)
\end{align*}
である。同様に $K_{X+Y}(t) = K_X(t) + K_Y(t)$ である。

## モーメントの母関数
::: {#thm-moment}

確率変数 $X$ について、ある $\epsilon >0$ が存在し、モーメント母関数 $M_X(t)$ が **$t\in(-\epsilon,\epsilon)$ で存在す
るとき**、

\begin{align*}
M_X(t) &=\sum_{n\ge 0}\frac{\expt{X^n}}{n!}t^n\qquad\forall t\in(-\epsilon,\epsilon)\\
\mu_n(X) &= \left.\frac{\mathrm{d}^n M_X(t)}{\mathrm{d} t^n}\right|_{t=0}.
\end{align*}
:::
::: {.proof}
前半の証明を与える。
離散型確率変数 $X$ について、
\begin{align*}
M_X(t) &= \expt{\mathrm{e}^{tX}}
 = \sum_x \mathrm{e}^{tx} f_X(x)
 = \sum_x \left(\sum_{n\ge0}\frac{(tx)^n}{n!}\right) f_X(x)
\end{align*}
である。
ここで、任意の $t\in(-\epsilon,\epsilon)$ について
\begin{align*}
 \sum_x \sum_{n\ge0}\left|\frac{(tx)^n}{n!} f_X(x)\right|
 &= \sum_x \sum_{n\ge0}\frac{|tx|^n}{n!} f_X(x)
 = \sum_x \mathrm{e}^{|tx|} f_X(x)
 \le \sum_x (\mathrm{e}^{tx}+\mathrm{e}^{-tx}) f_X(x)\\
 &= \expt{\mathrm{e}^{tX}} + \expt{\mathrm{e}^{-tX}}
 = M_X(t) + M_X(-t) < \infty
\end{align*}
よって、この無限和は任意の $t\in(-\epsilon,\epsilon)$ について**絶対収束する**。
そのため、和の順序を変えても収束値は変化しない。
\begin{align*}
M_X(t) &= 
 \sum_x \left(\sum_{n\ge0}\frac{(tx)^n}{n!}\right) f_X(x)
 = \sum_{n\ge 0} \sum_{x}\frac{(tx)^n}{n!}f_X(x)
 = \sum_{n\ge 0} \frac{\sum_xx^n f_X(x)}{n!}t^n
 = \sum_{n\ge 0} \frac{\expt{X^n}}{n!}t^n\qquad\forall t\in(-\epsilon,\epsilon).
\end{align*}
<!--
(概要)
\begin{align*}
M_X(t) &= \expt{\mathrm{e}^{tX}}\\
&=\expt{\sum_{n\ge 0}\frac{(tX)^n}{n!}}\\
&=\sum_{n\ge 0}\expt{\frac{(tX)^n}{n!}}\qquad\text{\textsf{(この無限和と期待値の交換が定理の条件より正当化される)}}.
\end{align*}
-->
:::

## キュムラント母関数
::: {#cor-cumulant}
確率変数 $X$ について、ある $\epsilon >0$ が存在し、モーメント母関数 $M_X(t)$ が **$t\in(-\epsilon,\epsilon)$ で存在するとき**、

\begin{align*}
\left.\frac{\mathrm{d} K_X(t)}{\mathrm{d} t}\right|_{t=0} &= \expt{X},&
\left.\frac{\mathrm{d}^2 K_X(t)}{\mathrm{d} t^2}\right|_{t=0} &=  \var{X}.
\end{align*}
:::
::: {.proof}
\begin{align*}
\left.\frac{\mathrm{d} K_X(t)}{\mathrm{d} t}\right|_{t=0} &= \left.\frac{M'_X(t)}{M_X(t)}\right|_{t=0}=\expt{X}\\
\left.\frac{\mathrm{d}^2 K_X(t)}{\mathrm{d} t^2}\right|_{t=0} &= \left.\frac{M''_X(t)M_X(t) - M'_X(t)^2}{M_X(t)^2}\right|_{t=0} = M''_X(0) - M'_X(0)^2 = \var{X}.
\end{align*}
:::

また、重要度は低くなるが、

\begin{align*}
\left.\frac{\mathrm{d}^3 K_X(t)}{\mathrm{d} t^3}\right|_{t=0} &= \expt{(X - \expt{X})^3}\\
\left.\frac{\mathrm{d}^4 K_X(t)}{\mathrm{d} t^4}\right|_{t=0} &= \expt{(X - \expt{X})^4} - 3\var{X}^2
\end{align*}

が成り立つ(覚えなくてよい)。
一般に以下を $X$ の **$n$ 次キュムラント** と呼ぶ。
\begin{align*}
\kappa_n(X) &\coloneqq \left.\frac{\mathrm{d}^n K_X(t)}{\mathrm{d} t^n}\right|_{t=0}.
\end{align*}

## モーメント母関数の一致
::: {#thm-moment}
確率変数 $X$ と $Y$ のモーメント母関数が0を含む開区間 $(-\epsilon,\,\epsilon)$ で存在し、それらが等しいとき、$X$ の分布と $Y$ の分布は等しい。
:::

証明は確率変数の像が有限の場合に与える(ウェブ資料参照)。

<!--
::: {.proof}
$\mathrm{Image}(X)$ と $\mathrm{Image}(Y)$ が有限の場合に限って証明を与える
(この場合はモーメント母関数は $\mathbb{R}$ 全体で存在するのだが)。
\begin{align*}
\{x_0,\dotsc,x_{N-1}\} &\coloneqq \mathrm{Image}(X) \cup \mathrm{Image}(Y)
\end{align*}
とする。
\begin{align*}
M_X(t) &= \sum_{k=0}^{N-1} f_X(x_k) \mathrm{e}^{tx_k},&
M_Y(t) &= \sum_{k=0}^{N-1} f_Y(x_k) \mathrm{e}^{tx_k}
\end{align*}
なので、
\begin{align*}
0 = M_X(t) - M_Y(t) &= \sum_{k=0}^{N-1} (f_X(x_k)-f_Y(x_k)) \mathrm{e}^{tx_k}\qquad\forall t\in(-\epsilon,\epsilon)
\end{align*}
各 $k\in\{0,1,\dotsc,N-1\}$ について、$t_k\coloneqq \epsilon\frac{k}{N}$ とおくと、
\begin{align}
\sum_{k=0}^{N-1} (f_X(x_k)-f_Y(x_k)) \mathrm{e}^{t_\ell x_k}&=0\qquad\forall \ell\in\{0,1,\dotsc,N-1\}
\end{align}
である。
ここで、$N\times N$ 実行列 $V$ を
\begin{align*}
V_{\ell k} &= \mathrm{e}^{t_\ell x_k}
= \mathrm{e}^{\frac{\epsilon x_k}{N} \ell}\qquad\forall k,\ell\in\{0,1,\dotsc,N-1\}
\end{align*}
とおく。
この行列 $V$ は Vandermonde行列の転置であり正則なので、
\begin{align*}
&\sum_{k=0}^{N-1} V_{\ell k} g_k=0\qquad\forall \ell\in\{0,1,\dotsc,N-1\}\\
\implies& g_k = 0 \qquad\forall k\in\{0,1,\dotsc,N-1\}
\end{align*}
よって、
\begin{align*}
f_X(x_k) &= f_Y(x_k) \qquad\forall k\in\{0,1,\dotsc,N-1\}
\end{align*}
である。
:::
-->
@thm-moment より、モーメント母関数には確率変数の分布のすべての情報が含まれていると言える。
ただし、モーメント母関数は原点まわりで存在しないこともあるので、分布の情報をすべて含む関数としては**特性関数**
\begin{align*}
\varphi_X(t) &\coloneqq \expt{\mathrm{e}^{itX}}\qquad\forall t\in\mathbb{R}
\end{align*}
の方が優秀である。
特性関数は常に存在する。
一方でモーメント母関数は確率の集中を示す文脈では中心的な役割を果たす。




## 今週の課題


$\lambda>0$ について $X\sim\mathrm{Poisson}(\lambda)$ とする。
以下の問に答えよ

::: {.wide-list}

1. $X$ のキュムラント母関数をもとめよ。
1. $\expt{X}$ と $\var{X}$ をもとめよ。

:::
